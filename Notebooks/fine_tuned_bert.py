# -*- coding: utf-8 -*-
"""Fine_tuned_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iR7YHxbhsYYIg2QiveeJJ_kr6a3tuddz

```
Augusto Sousa Nunes - NºUSP: 11319677
Lua Nardi Quito - NºUSP: 11371270
Lucas Schimidt Coelho - NºUSP: 11913019
Lucas Garzeri de Melo - NºUSP: 13731344
Lucca Baptista Silva Ferraz - NºUSP: 13688134
```

#Imports e dados
"""

# Dependencias base
import pandas as pd
import  matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import re
import statsmodels.api as sm

# Análise linguística
!python -m spacy download pt_core_news_sm
import spacy
nlp = spacy.load("pt_core_news_sm")
!pip install pyphen
import pyphen

# Embeddings - o gensim só é compatível com versões antigas do numpy, utiliza-lo apenas quando for mexer com os embeddings word2vec
'''!pip install numpy==1.24.3
import numpy as np
!pip install -q --upgrade gensim
from gensim.models import KeyedVectors'''
# Bertimbau
import numpy as np
from sentence_transformers import SentenceTransformer

# UMAP para visualização - só funciona com versões atualizadas do numpy, usar apenas quando não for mexer nos embeddings do gensim
!pip install umap-learn
import umap

# Métricas
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

nlp = spacy.load("pt_core_news_sm")
dic = pyphen.Pyphen(lang='pt_BR')

# Drive
from google.colab import drive
drive.mount('/content/drive', force_remount = True)

# Warnings
import warnings
warnings.filterwarnings('ignore')


# Wordcloud
from wordcloud import WordCloud

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

"""---

## **Base de dados estruturada**
"""

#@title Extração de dados

df = pd.read_csv('/content/drive/MyDrive/Dados ENEM Mineração/Base - Lucca/csv_final.csv')
df.head()

df.info()

"""---

---
# **Fine tuned BERT**
"""

#@title Torch imports
!pip install datasets

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
import torch.nn as nn
import tqdm
import transformers as ppb
from transformers import AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments, AutoModelForSequenceClassification
from datasets import Dataset

"""## **Classes de regressão e early stopping**

#### **Reinitialize layers e AdamW**



```
Uma boa prática ao realizar o fine tuning de arquiteturas Transformer é reinicializar as camadas próximas à
saída. Além disso, substitui-se o otimizador AdamBERT pelo AdamW, que integra um termo de correção de viés na
atualização dos pesos
```
"""

#@title Model e tokenizer

model = ppb.AutoModelForSequenceClassification.from_pretrained(
    'neuralmind/bert-base-portuguese-cased',
    num_labels=1,
    problem_type="regression",
    output_attentions=False,
    output_hidden_states=False
)


def reinitialize_last_layers(model, num_layers=2):
    for layer in model.bert.encoder.layer[-num_layers:]:
        for module in layer.modules():
            if isinstance(module, nn.Linear):
                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)
                if module.bias is not None:
                    module.bias.data.zero_()
            elif isinstance(module, nn.Embedding):
                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)
            elif isinstance(module, nn.LayerNorm):
                module.bias.data.zero_()
                module.weight.data.fill_(1.0)
    if hasattr(model, "classifier"):
        model.classifier.weight.data.normal_(mean=0.0, std=model.config.initializer_range)
        model.classifier.bias.data.zero_()


reinitialize_last_layers(model, 2)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=2.26e-5, eps=5e-08, weight_decay=0.03)
tokenizer = ppb.AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)

#@title Pre-processamento

train_df, test_df = train_test_split(df_regressao, test_size=0.2, random_state=24)

def preprocessing(input_text, tokenizer):
    return tokenizer.encode_plus(
        input_text,
        add_special_tokens=True,
        max_length=512,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt'
    )


def tokenize_dataframe(df, tokenizer):
    tokenized = []
    attention_masks = []
    for sample in df['enunciado'].values:
        encoding_dict = preprocessing(sample, tokenizer)
        tokenized.append(encoding_dict['input_ids'])
        attention_masks.append(encoding_dict['attention_mask'])
    tokenized = torch.cat(tokenized, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    return tokenized, attention_masks

train_input_ids, train_attention_masks = tokenize_dataframe(train_df, tokenizer)
test_input_ids, test_attention_masks = tokenize_dataframe(test_df, tokenizer)

# Conversão para float está dando errado, ajustar eventualmente
train_labels = torch.tensor(train_df['NU_PARAM_B'].values, dtype=torch.float)
test_labels = torch.tensor(test_df['NU_PARAM_B'].values, dtype=torch.float)


train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)
test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)

train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)
test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=16)

#@title Treinamento com early stopping class

num_epochs = 3
best_val_loss = float('inf')
patience = 2
early_stop_counter = 0

for epoch in range(num_epochs):
    model.train()
    total_train_loss = 0
    for batch in tqdm.tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs} training"):
        b_input_ids, b_input_mask, b_labels = (t.to(device) for t in batch)
        optimizer.zero_grad()

        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels.unsqueeze(1))
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_dataloader)

    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for batch in tqdm.tqdm(test_dataloader, desc="Validation"):
            b_input_ids, b_input_mask, b_labels = (t.to(device) for t in batch)
            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels.unsqueeze(1))
            total_val_loss += outputs.loss.item()
    avg_val_loss = total_val_loss / len(test_dataloader)
    print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        early_stop_counter = 0
    else:
        early_stop_counter += 1
        if early_stop_counter >= patience:
            print("Early stopping!")
            break

#@title Eval
model.eval()
y_preds = []
y_true = []
with torch.no_grad():
    for batch in test_dataloader:
        b_input_ids, b_input_mask, b_labels = (t.to(device) for t in batch)
        outputs = model(b_input_ids, attention_mask=b_input_mask)
        preds = outputs.logits.squeeze(1)  # shape: (batch_size)
        y_preds.extend(preds.cpu().numpy())
        y_true.extend(b_labels.cpu().numpy())

y_preds = np.array(y_preds)
y_true = np.array(y_true)

from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_true, y_preds)
r2 = r2_score(y_true, y_preds)
corr = np.corrcoef(y_true, y_preds)[0, 1]

print("MSE:", mse)
print("Correlação:", corr)

"""---

## **Sem reinicializar as primeiras camadas**

---
## **Otimizando hiperparâmetros**
"""

#@title Dependências
!pip install optuna
import optuna

#@title Optuna

model = "neuralmind/bert-base-portuguese-cased"
tokenizer = AutoTokenizer.from_pretrained(model)
data_collator = DataCollatorWithPadding(tokenizer, padding="longest")

train_df, test_df = train_test_split(df_regressao, test_size=0.2, random_state=24)

train_ds = Dataset.from_pandas(train_df.rename(columns={"NU_PARAM_B":"label"}))
eval_ds  = Dataset.from_pandas(test_df.rename(columns={"NU_PARAM_B":"label"}))

def tokenize_function(examples):
    return tokenizer(examples["enunciado"],
                     truncation=True,
                     max_length=512,
                     padding=False)

train_ds = train_ds.map(tokenize_function, batched=True)
eval_ds  = eval_ds.map(tokenize_function,  batched=True)

def compute_metrics(p):
    preds  = p.predictions.squeeze()
    labels = p.label_ids
    corr   = np.corrcoef(labels, preds)[0,1]
    mse    = mean_squared_error(labels, preds)
    return {"correlation": corr, "mse": mse}

"""def compute_objective(trial):
    preds = trial.predict.squeeze()
    labels = trial.label_ids

    mse = mean_squared_error(labels, preds)

    return mse"""

def model_init():

    model = AutoModelForSequenceClassification.from_pretrained(
        "neuralmind/bert-base-portuguese-cased",
        num_labels=1
    )

    reinitialize_last_layers(model, num_layers=2)

    return model

# Não salva os logs
training_args = TrainingArguments(
    output_dir="./dummy",
    do_train=True,
    do_eval=True,
    save_strategy="no",
    logging_strategy="no",
    report_to=[],
    disable_tqdm=False,
)

trainer = Trainer(
    model_init     = model_init,
    args           = training_args,
    train_dataset  = train_ds,
    eval_dataset   = eval_ds,
    tokenizer      = tokenizer,
    data_collator  = data_collator,
    compute_metrics= compute_metrics
)

def hp_space(trial: optuna.Trial):
    return {
        "learning_rate":  trial.suggest_loguniform("learning_rate", 1e-5, 5e-5),
        "weight_decay":   trial.suggest_float("weight_decay",    0.0,   0.2),
    }

best_run = trainer.hyperparameter_search(
    direction        = "minimize",
    backend          = "optuna",
    hp_space         = hp_space,
    n_trials         = 20,
    compute_objective = lambda metrics: metrics["eval_mse"]
)

print("Melhor learning_rate :", best_run.hyperparameters["learning_rate"])
print("Melhor weight_decay :", best_run.hyperparameters["weight_decay"])
print("Melhor correlação   :", best_run.objective)

#@title Calculando métricas de avaliação

best_hps = best_run.hyperparameters
best_lr = best_hps["learning_rate"]
best_wd = best_hps["weight_decay"]

training_args = TrainingArguments(
    output_dir="./dummy",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=best_lr,
    weight_decay=best_wd,
    disable_tqdm=False,
    report_to='none'
)


trainer = Trainer(
    model_init     = model_init,
    args           = training_args,
    train_dataset  = train_ds,
    eval_dataset   = eval_ds,
    tokenizer      = tokenizer,
    data_collator  = data_collator,
    compute_metrics= compute_metrics
)

trainer.train()

predictions_output = trainer.predict(eval_ds)
preds = predictions_output.predictions.squeeze()   # array de floats
labels = predictions_output.label_ids


print("MSE final:", mean_squared_error(labels, preds))
print("R² final: ", r2_score(labels, preds))
print("Corr final:", np.corrcoef(labels, preds)[0,1])

"""---
### **Multilingual BERT**
"""

#@title Optuna

model = "google-bert/bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(model)
data_collator = DataCollatorWithPadding(tokenizer, padding="longest")

train_df, test_df = train_test_split(df_regressao, test_size=0.2, random_state=24)

train_ds = Dataset.from_pandas(train_df.rename(columns={"NU_PARAM_B":"label"}))
eval_ds  = Dataset.from_pandas(test_df.rename(columns={"NU_PARAM_B":"label"}))

def tokenize_function(examples):
    return tokenizer(examples["enunciado"],
                     truncation=True,
                     max_length=512,
                     padding=False)

train_ds = train_ds.map(tokenize_function, batched=True)
eval_ds  = eval_ds.map(tokenize_function,  batched=True)

def compute_metrics(p):
    preds  = p.predictions.squeeze()
    labels = p.label_ids
    corr   = np.corrcoef(labels, preds)[0,1]
    mse    = mean_squared_error(labels, preds)
    return {"correlation": corr, "mse": mse}

"""def compute_objective(trial):
    preds = trial.predict.squeeze()
    labels = trial.label_ids

    mse = mean_squared_error(labels, preds)

    return mse"""

def model_init():

    model = AutoModelForSequenceClassification.from_pretrained(
        "google-bert/bert-base-multilingual-cased",
        num_labels=1
    )

    reinitialize_last_layers(model, num_layers=2)

    return model

training_args = TrainingArguments(
    output_dir="./dummy",
    do_train=True,
    do_eval=True,
    save_strategy="no",
    logging_strategy="no",
    report_to=[],
    disable_tqdm=False,
)

trainer = Trainer(
    model_init     = model_init,
    args           = training_args,
    train_dataset  = train_ds,
    eval_dataset   = eval_ds,
    tokenizer      = tokenizer,
    data_collator  = data_collator,
    compute_metrics= compute_metrics
)

def hp_space(trial: optuna.Trial):
    return {
        "learning_rate":  trial.suggest_loguniform("learning_rate", 1e-5, 5e-5),
        "weight_decay":   trial.suggest_float("weight_decay",    0.0,   0.2),
    }

best_run = trainer.hyperparameter_search(
    direction        = "minimize",
    backend          = "optuna",
    hp_space         = hp_space,
    n_trials         = 20,
    compute_objective = lambda metrics: metrics["eval_mse"]
)

print("Melhor learning_rate :", best_run.hyperparameters["learning_rate"])
print("Melhor weight_decay :", best_run.hyperparameters["weight_decay"])
print("Melhor correlação   :", best_run.objective)

#@title Extraindo métricas de avaliação

def model_init():

    model = AutoModelForSequenceClassification.from_pretrained(
        "google-bert/bert-base-multilingual-cased",
        num_labels=1
    )

    reinitialize_last_layers(model, num_layers=2)

    return model

best_hps = best_run.hyperparameters
best_lr = best_hps["learning_rate"]
best_wd = best_hps["weight_decay"]

training_args = TrainingArguments(
    output_dir="./dummy",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=best_lr,
    weight_decay=best_wd,
    disable_tqdm=False,
    report_to='none'
)


trainer = Trainer(
    model_init     = model_init,
    args           = training_args,
    train_dataset  = train_ds,
    eval_dataset   = eval_ds,
    tokenizer      = tokenizer,
    data_collator  = data_collator,
    compute_metrics= compute_metrics
)

trainer.train()

predictions_output = trainer.predict(eval_ds)
preds = predictions_output.predictions.squeeze()   # array de floats
labels = predictions_output.label_ids


print("MSE final:", mean_squared_error(labels, preds))
print("R² final: ", r2_score(labels, preds))
print("Corr final:", np.corrcoef(labels, preds)[0,1])