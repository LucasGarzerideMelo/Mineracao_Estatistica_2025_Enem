# -*- coding: utf-8 -*-
"""Artigo - Dados Atualizados.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mXQoWRE_xUsANld7hScf05RMFkinHzk7

---

# **Mineração Estatística de Dados - Projeto Enem**

```
Augusto Sousa Nunes - NºUSP: 11319677
Lua Nardi Quito - NºUSP: 11371270
Lucas Schimidt Coelho - NºUSP: 11913019
Lucas Garzeri de Melo - NºUSP: 13731344
Lucca Baptista Silva Ferraz - NºUSP: 13688134
```
"""

#@title Imports

# Dependencias base
import pandas as pd
import  matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import re
import statsmodels.api as sm

# Análise linguística
!python -m spacy download pt_core_news_sm
import spacy
nlp = spacy.load("pt_core_news_sm")
!pip install pyphen
import pyphen

# Embeddings - o gensim só é compatível com versões antigas do numpy, utiliza-lo apenas quando for mexer com os embeddings word2vec
'''!pip install numpy==1.24.3
import numpy as np
!pip install -q --upgrade gensim
from gensim.models import KeyedVectors'''
# Bertimbau
import numpy as np
from sentence_transformers import SentenceTransformer

# UMAP para visualização - só funciona com versões atualizadas do numpy, usar apenas quando não for mexer nos embeddings do gensim
!pip install umap-learn
import umap

# Métricas
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

nlp = spacy.load("pt_core_news_sm")
dic = pyphen.Pyphen(lang='pt_BR')

# Drive
from google.colab import drive
drive.mount('/content/drive', force_remount = True)

# Warnings
import warnings
warnings.filterwarnings('ignore')


# Wordcloud
from wordcloud import WordCloud

"""---

## **Base de dados estruturada**
"""

#@title Extração de dados

df = pd.read_csv('/content/drive/MyDrive/Mineração Estatística/Dados ENEM Mineração/Base - Lucca/csv_final.csv')
df.head()

"""---
# **1. Extração de features linguísticos**

A fim de iniciar a análise exploratória das questões do Enem, foram extraídos atributos textuais dos enunciados das questões.As métricas aqui calculadas se baseiam no artigo do NILC "*Sidney Evaldo Leal, Magali Sanches Duran, Carolina Evaristo Scarton, Nathan Siegle Hartmann, Sandra Maria Aluísio. NILC-Metrix: assessing the complexity of written and spoken language in Brazilian Portuguese. Lang Resources & Evaluation (2023)*" e avaliam características como inteligibilidade, complexidade textual, coesão e coerência. Todas as métricas foram adaptadas às nuances da língua portuguesa e foram implementadas à partir da biblioteca **spaCy**, própria para análise morfossintática.

---

#### **A seguir, uma breve descrição das métricas extraídas:**

**1. Type-token ratio**

Proporção de palavras sem repetições (types) em relação ao total de palavras com repetições (tokens).

**2. Number of sentences**

Número de sentenças.

**3. Size of sentences in words**

Tamanho médio das sentenças do enunciado.

**4. Verb-token ratio**

Proporção do número de verbos em relação ao número de tokens.

**5. Noun-token ratio**

Proporção do número de substantivos em relação ao número de tokens.

**6. Adjective-token ratio**

Proporção do número de adjetivos em relação ao número de tokens.

**7. Pronoun-token ratio**

Proporção do número de pronomes em relação ao número de tokens.

**8. Stopword-token ratio**

Proporção de stopwords (palavras que não carregam significado semântico) em relação ao número de tokens.

**9. Pausality**

Quantifica a presença de pausas ou interrupções na fluidez de um texto. Proporção de sinais de pontuação em relação ao total de tokens.

**10. Índice de Brunet**

Métrica de complexidade textual que avalia a proporção type-token de forma menos sensível ao tamanho de texto. Sua fórmula é descrita como:

$$
\text{Brunet's Index} = N^{V^{-0.165}}
$$

onde:
- \(N\) é o número total de palavras (tokens) no texto,
- \(V\) é o tamanho do vocabulário (número de palavras únicas).
'''

\\

**11. Índice de gunning-fog**

O índice de inteligibilidade Gunning Fog soma a quantidade média de palavras por sentença ao percentual de palavras difíceis no texto e multiplica tudo por 0.4. O resultado está diretamente ligado aos 12 níveis do ensino americano. Índices superiores a 12 representam textos extremamente complexos.

[1] *LEAL, Sidney Evaldo; DURAN, Magali Sanches; SCARTON, Carolina Evaristo; HARTMANN, Nathan Siegle; ALUÍSIO, Sandra Maria. NILC-Metrix: assessing the complexity of written and spoken language in Brazilian Portuguese. 2021. Preprint. Disponível em: https://arxiv.org/abs/2201.03445.*

---
"""

#@title Funções de extração de features linguísticos

nlp = spacy.load("pt_core_news_sm")
dic = pyphen.Pyphen(lang='pt_BR')
palavras_familiares = nlp.Defaults.stop_words

def contar_silabas(palavra):
    s = dic.inserted(palavra)
    return 1 if '-' not in s else len(s.split('-'))

def eh_palavra_dificil(token):
    if not token.is_alpha:
        return False
    if token.pos_ == "PROPN":
        return False
    if '-' in token.text:
        return False
    if token.text.lower() in palavras_familiares:
        return False
    return contar_silabas(token.text) >= 3

def extrair_metricas_completas(texto):
    doc = nlp(texto)
    tokens_ling = [t for t in doc if not t.is_space and not t.is_punct]
    total_tokens_ling = len(tokens_ling)
    words_alpha_ling = [t.text.lower() for t in tokens_ling if t.is_alpha]
    unique_words_ling = set(words_alpha_ling)
    type_token_ratio = len(unique_words_ling) / total_tokens_ling if total_tokens_ling > 0 else 0
    num_sentences = len(list(doc.sents))
    size_of_sentences_in_words = total_tokens_ling / num_sentences if num_sentences > 0 else 0
    num_verbs = sum(1 for t in tokens_ling if t.pos_ == "VERB")
    num_nouns = sum(1 for t in tokens_ling if t.pos_ in ("NOUN", "PROPN"))
    num_adjs = sum(1 for t in tokens_ling if t.pos_ == "ADJ")
    num_advs = sum(1 for t in tokens_ling if t.pos_ == "ADV")
    num_pron = sum(1 for t in tokens_ling if t.pos_ == "PRON")
    verb_to_token_ratio = num_verbs / total_tokens_ling if total_tokens_ling > 0 else 0
    noun_to_token_ratio = num_nouns / total_tokens_ling if total_tokens_ling > 0 else 0
    adjective_to_token_ratio = num_adjs / total_tokens_ling if total_tokens_ling > 0 else 0
    adverb_to_token_ratio = num_advs / total_tokens_ling if total_tokens_ling > 0 else 0
    pronoun_to_token_ratio = num_pron / total_tokens_ling if total_tokens_ling > 0 else 0
    num_stopwords = sum(1 for t in tokens_ling if t.is_stop)
    stopword_to_token_ratio = num_stopwords / total_tokens_ling if total_tokens_ling > 0 else 0
    punct_tokens = [t for t in doc if t.is_punct]
    total_with_punct = total_tokens_ling + len(punct_tokens)
    pausality = len(punct_tokens) / total_with_punct if total_with_punct > 0 else 0

    tokens_no_space = [t for t in doc if not t.is_space]
    total_tokens_no_space = len(tokens_no_space)
    tokens_palavra = [t for t in doc if t.is_alpha]
    total_palavras = len(tokens_palavra)
    num_palavras_complexas = sum(1 for t in tokens_palavra if eh_palavra_dificil(t))
    num_sentencas = len(list(doc.sents))
    media_tokens_sentenca = total_palavras / num_sentencas if num_sentencas > 0 else 0
    total_silabas = sum(contar_silabas(t.text) for t in tokens_palavra)
    media_silabas_palavra = total_silabas / total_palavras if total_palavras > 0 else 0
    percentual_palavras_complexas = (num_palavras_complexas / total_palavras) * 100 if total_palavras > 0 else 0
    gunning_fog = 0.4 * (media_tokens_sentenca + percentual_palavras_complexas / 100) if total_palavras > 0 else 0
    vocab_palavras = set(t.text.lower() for t in tokens_palavra)
    vocab_size = len(vocab_palavras)
    brunet = total_palavras ** (vocab_size ** (-0.165)) if total_palavras > 0 and vocab_size > 0 else 0

    return {
        "type_token_ratio": type_token_ratio,
        "number_of_sentences": num_sentences,
        "size_of_sentences_in_words": size_of_sentences_in_words,
        "verb_to_token_ratio": verb_to_token_ratio,
        "noun_to_token_ratio": noun_to_token_ratio,
        "adjective_to_token_ratio": adjective_to_token_ratio,
        "adverb_to_token_ratio": adverb_to_token_ratio,
        "pronoun_to_token_ratio": pronoun_to_token_ratio,
        "stopword_to_token_ratio": stopword_to_token_ratio,
        "pausality": pausality,
        "num_tokens": total_tokens_no_space,
        "num_sentencas": num_sentencas,
        "media_tokens_sentenca": media_tokens_sentenca,
        "total_silabas": total_silabas,
        "media_silabas_palavra": media_silabas_palavra,
        "num_palavras": total_palavras,
        "num_palavras_complexas": num_palavras_complexas,
        "percentual_palavras_complexas": percentual_palavras_complexas,
        "gunning_fog": gunning_fog,
        "brunet": brunet
    }

#@title Exemplo Spacy

texto_exemplo = df['enunciado'][3]

doc = nlp(texto_exemplo)

print('-----------------------------------------------------------------------')
print('TEXTO:', texto_exemplo)
print('-----------------------------------------------------------------------')

print('\n')

for token in doc[:3]:
    print('-----------------------------------------------------------------------')
    print('TOKEN:', token.text, '; LEMA:', token.lemma_, '; POS:', token.pos_, '; TAG:', token.tag_, '; DEP:', token.dep_)

print('-----------------------------------------------------------------------')

#@title Exemplo

texto_exemplo = (
    '''   O acessório polêmico entrou no projeto, de autoria do senador Cícero Lucena (PSDB-PB), graças a uma emenda
    aprovada na Comissão de Educação do Senado em outubro. Foi o senador Flávio Arns (PT-PR) quem sugeriu a
    inclusão da peça entre os itens do uniforme de alunos dos ensinos Fundamental e Médio nas escolas municipais,
    estaduais e federais. Ele defende a medida como forma de proteger crianças e adolescentes dos males provocados
    pelo excesso de exposição aos raios solares. Se a ideia for aprovada, os estudantes receberão dois conjuntos
    anuais, completados por calçado, meias, calça e camiseta.''')

print("------------------------------------------------------------------------------------------------------------------")
print("TEXTO EXEMPLO:\n", texto_exemplo)
print("------------------------------------------------------------------------------------------------------------------")
print("\n")


print("------------------------------------------------------------------------------------------------------------------")
print("METRICAS:")
resultado = extrair_metricas_completas(texto_exemplo)
for chave, valor in resultado.items():
    if isinstance(valor, float):
        print(f"{chave}: {valor:.2f}")
    else:
        print(f"{chave}: {valor}")

print("------------------------------------------------------------------------------------------------------------------")

#@title Aplicando ao dataframe

# Não devemos fazer isso, está aqui provisóriamente até que todos os enunciados sejam tratados
metrics_series = df['enunciado'].astype(str).apply(lambda x: extrair_metricas_completas(x))


metrics_df = pd.DataFrame(metrics_series.tolist())
df_final = pd.concat([df, metrics_df], axis=1)
df_final['NU_PARAM_B'] = df['NU_PARAM_B']

df_final.head()

#@title Correlação com o parâmetro de dificuldade B

df_corr = df_final[['type_token_ratio', 'number_of_sentences', 'size_of_sentences_in_words',
       'verb_to_token_ratio', 'noun_to_token_ratio',
       'adjective_to_token_ratio', 'adverb_to_token_ratio',
       'pronoun_to_token_ratio', 'stopword_to_token_ratio', 'pausality',
       'num_tokens', 'num_sentencas', 'media_tokens_sentenca', 'total_silabas',
       'media_silabas_palavra', 'num_palavras', 'num_palavras_complexas',
       'percentual_palavras_complexas', 'gunning_fog', 'brunet', 'NU_PARAM_B']]

corr_matrix = df_corr.corr()
corr_with_param_b = corr_matrix['NU_PARAM_B'].drop('NU_PARAM_B')

print("Correlação com NU_PARAM_B:")
print(corr_with_param_b)

"""---

# **2. NILC Embeddings**

Conforme a metodologia do artigo, os enunciados das questões do Enem foram vetorizados a partir do algoritmo Word2Vec, extraindo a média dos vetores de palavra para cada item. O algoritmo Word2Vec é uma técnica de aprendizado de representações vetoriais para palavras, cujo funcionamento depende de uma rede neural rasa que recebe a tarefa de reconstruir contexto linguístico. Assim, os modelos aprendem a criar representações de alta dimensão que preservam a semântica das palavras de forma implícita.

\\

**Existem duas arquiteturas principais em Word2Vec:**

#### *CBOW (Continuous Bag of Words):*
Nesse modelo, o algoritmo prevê a palavra central a partir das palavras de contexto ao redor. Assim, o modelo aprende quais palavras costumam aparecer juntas, aproximando os vetores de palavras que compartilham contextos similares.

#### *Skip-gram:*
Nesse caso, o processo é invertido: a partir de uma palavra central, o modelo tenta prever as palavras que aparecem em seu contexto. Essa abordagem é especialmente eficaz para capturar relações semânticas em conjuntos de dados menores.

Todos os vetores aqui utilizados foram retirados do repositório do NILC.

[1] *HARTMANN, Nathan; FONSECA, Erick; SHULBY, Christopher; TREVISO, Marcos; RODRIGUES, Jessica; ALUISIO, Sandra. Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks. 2017. Preprint. Disponível em: https://arxiv.org/abs/1708.06025.*
"""

!python -m spacy init vectors pt "/content/drive/MyDrive/Mineração Estatística/NILC - Embeddings/cbow_s300.txt" output_vectors

nlp_vectors = spacy.load("output_vectors")
print("Dimensão dos vetores:", nlp_vectors.vocab.vectors_length)

#@title Gerar embeddings dos enunciados


def get_embedding_enunciado(text, nlp_model = nlp_vectors):
    doc = nlp_model(text.lower())
    vectors = [token.vector for token in doc if token.has_vector]
    return np.mean(vectors, axis=0) if vectors else np.zeros(nlp_model.vocab.vectors_length)

#@title Vetorização segundo o protocolo primi

def preprocess_item(text):
    doc = nlp(text)
    words = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]
    seen = set()
    unique_words = []
    for word in words:
        if word not in seen:
            unique_words.append(word)
            seen.add(word)
    return unique_words

def get_avg_embedding(tokens, nlp_model=nlp_vectors):
    vectors = []
    for token in tokens:
        if nlp_model.vocab.has_vector(token):
            vectors.append(nlp_model.vocab[token].vector)
    if vectors:
        return np.mean(vectors, axis=0)
    return np.zeros(nlp_model.vocab.vectors_length)

#@title Aplicando à coluna "Enunciados"

embeddings_df = pd.DataFrame()
embeddings_df['enunciado'] = df['enunciado']
embeddings_df['NU_PARAM_B'] = df['NU_PARAM_B']
embeddings_df['embeddings'] = embeddings_df['enunciado'].astype(str).apply(get_embedding_enunciado)
embeddings_df

"""## **Uniform Manifold Approximation and Projection (UMAP)**

O UMAP é um método de Redução de dimensionalidade que visa preservar cluster de altas dimensões em um gráfico de baixa dimensão. Facilita-se, pois, a visualização dos embeddings gerados a partir do Word2Vec, uma vez que a codificação do parâmetro de dificuldade é feita através da cor dos pontos no espaço de dimensionalidade reduzida.
"""

#@title Aplicando o UMAP

df_filtered = embeddings_df[(embeddings_df["NU_PARAM_B"] >= -3) & (embeddings_df["NU_PARAM_B"] <= 3)].copy()
print("Número de questões filtradas:", len(df_filtered))

embeddings_matrix = np.vstack(df_filtered["embeddings"].values)
print("Dimensão dos embeddings filtrados:", embeddings_matrix.shape)

reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
umap_embeddings = reducer.fit_transform(embeddings_matrix)
print("Dimensão reduzida:", umap_embeddings.shape)

#@title UMAP Scatterplot
import plotly.graph_objects as go

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=umap_embeddings[:, 0],
    y=umap_embeddings[:, 1],
    mode='markers',
    marker=dict(
        size=7,
        color=df_filtered['NU_PARAM_B'],
        colorscale= 'RdBu',
        colorbar=dict(title='NU_PARAM_B')
    ),
    hoverinfo='text',
    text=embeddings_df['NU_PARAM_B'].astype(str)
))

fig.update_layout(
    title="Projeção UMAP dos Embeddings",
    xaxis_title="UMAP 1",
    yaxis_title="UMAP 2",
    template="plotly_white"
)

fig.show()

"""---

# **3. BERT embeddings**

De maneira análoga à extração de embeddings realizada pelo Word2Vec, utilizou-se dos enunciados das questões através do BERT [Devlin et al., 2019] (Bidirectional Encoder Representaions from Transformers), uma família de modelos de linguagem projetada com a habilidade de capturar contexto em duas direções, superando a performance de modelos contemporâneos a ele. Aqui, vale o destaque para o fato de que o modelo é sentencial, isto é, não mais depende da média das dimensões dos vetores palavra a palavra, captando sentenças completas e, por consequência, relações de ordem e posição entre as palavras nas frases.

Neste estudo, mais especificamente, utiliza-se do Bertimbau [Souza et al., 2020], uma variante treinada especificamente para o português brasileiro, que se adequa melhor às nuances da língua e cultura brasileira. Os embeddings extraídos pelo BERT serão, posteriormente, utilizados como atributos para a regressão do parâmetro de dificuldade dos itens.


[1] *Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
 BERT: Pre-training of deep bidirectional transformers for
 language understanding. In Burstein, J., Doran, C., and
 Solorio, T., editors, Proceedings of the 2019 Conference
 of the NorthAmericanChapteroftheAssociationforCom
putational Linguistics: Human Language Technologies,
 Volume 1 (Long and Short Papers), pages 4171–4186,
 Minneapolis, Minnesota.*

[2] *Souza, F., Nogueira, R., and Lotufo, R. (2020). BERTim
bau: pretrained BERT models for Brazilian Portuguese.
 In Cerri, R. and Prati, R. C., editors, Proceedings
 of the 9th Brazilian Conference on Intelligent Systems,
 BRACIS,RioGrandedoSul, Brazil, October20-23, pages
 403–417, Cham. Springer International Publishing. DOI:
 10.1007/978-3-030-61377-8_28.*
"""

#@title Bert embeddings
bert_model = SentenceTransformer("neuralmind/bert-base-portuguese-cased")

def get_bert_embedding(text, model):
    return model.encode(text)

bert_embeddings = df.copy()
bert_embeddings['embeddings'] = bert_embeddings['enunciado'].astype(str).apply(lambda x: get_bert_embedding(x, bert_model))

#@title Aplicando o UMAP

df_filtered_bert = bert_embeddings[(bert_embeddings["NU_PARAM_B"] >= -3) & (bert_embeddings["NU_PARAM_B"] <= 3)].copy()
print("Número de questões filtradas:", len(df_filtered_bert))

embeddings_matrix = np.vstack(df_filtered_bert["embeddings"].values)
print("Dimensão dos embeddings filtrados:", embeddings_matrix.shape)

reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
umap_embeddings = reducer.fit_transform(embeddings_matrix)
print("Dimensão reduzida:", umap_embeddings.shape)

#@title UMAP Scatterplot

import plotly.graph_objects as go

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=umap_embeddings[:, 0],
    y=umap_embeddings[:, 1],
    mode='markers',
    marker=dict(
        size=7,
        color=df_filtered_bert['NU_PARAM_B'],
        colorscale= 'RdBu',
        colorbar=dict(title='NU_PARAM_B')
    ),
    hoverinfo='text',
    text=df_filtered_bert['NU_PARAM_B'].astype(str)
))

fig.update_layout(
    title="Projeção UMAP dos Embeddings",
    xaxis_title="UMAP 1",
    yaxis_title="UMAP 2",
    template="plotly_white"
)

fig.show()

"""---

# **4. Similaridade entre os embeddings das alternativas**
"""

#@title Similaridade entre os embeddings

bert_model = SentenceTransformer("neuralmind/bert-base-portuguese-cased")

def get_bert_embedding(text, model):
    return model.encode(text)

def average_alternatives_similarity(row, alternatives_columns):
    embeddings = []
    for col in alternatives_columns:
        alt_text = row[col]
        if isinstance(alt_text, str) and alt_text.strip():
            emb = get_bert_embedding(alt_text, bert_model)
            embeddings.append(emb)
    if len(embeddings) < 2:
        return np.nan
    embeddings = np.vstack(embeddings)
    sim_matrix = cosine_similarity(embeddings)
    n = embeddings.shape[0]
    avg_sim = (np.sum(sim_matrix) - n) / (n * (n - 1))
    return avg_sim

df_similaridades = bert_embeddings[(bert_embeddings["NU_PARAM_B"] >= -3) & (bert_embeddings["NU_PARAM_B"] <= 3)].copy()
alternatives_columns = ['A', 'B', 'C', 'D', 'E']
df_similaridades["avg_alt_similarity"] = df_similaridades.apply(lambda row: average_alternatives_similarity(row, alternatives_columns), axis=1)
df_similaridades.head()

#@title Correlação: média das similariades x Parâmetro B

df_similaridades[['avg_alt_similarity', 'NU_PARAM_B']].corr()

#@title Scatterplot: média das similariades x Parâmetro B


fig = go.Figure()

fig.add_trace(go.Scatter(
    x=df_similaridades['NU_PARAM_B'],
    y=df_similaridades['avg_alt_similarity'],
    mode='markers',
    marker=dict(
        size=7,
    ),
    hoverinfo='text',
))

fig.update_layout(
    title="Distribuição: média das similariades x Parâmetro B",
    xaxis_title="Parâmetro de dificuldade do item",
    yaxis_title="Média da similaridade entre alternativas",
    template="plotly_white"
)

fig.show()

"""***"""

#@title Similaridade entre o gabarito e o enunciado

def similarity_gabarito_enunciado(row, model):
    correct_letter = row["TX_GABARITO"].strip().lower()
    if correct_letter.upper() in ['A', 'B', 'C', 'D', 'E']:
        alternative_text = row[correct_letter.upper()]
        if isinstance(alternative_text, float):
              alternative_text = str(alternative_text)
    else:
        return np.nan

    embedding_enunciado = get_bert_embedding(row["enunciado"], model)
    embedding_alternative = get_bert_embedding(alternative_text, model)

    norm_enunciado = np.linalg.norm(embedding_enunciado)
    norm_alternative = np.linalg.norm(embedding_alternative)
    if norm_enunciado == 0 or norm_alternative == 0:
        return np.nan

    cosine_sim = np.dot(embedding_enunciado, embedding_alternative) / (norm_enunciado * norm_alternative)
    return cosine_sim

df_similaridades["gabarito_similarity"] = df_similaridades.apply(lambda row: similarity_gabarito_enunciado(row, bert_model), axis=1)
df_similaridades[["TX_GABARITO", "enunciado", "gabarito_similarity"]].head()

#@title Correlação: similariade enunciado-gabarito x Parâmetro B

df_similaridades[['gabarito_similarity', 'NU_PARAM_B']].corr()

#@title Scatterplot: média das similariades x Parâmetro B


fig = go.Figure()

fig.add_trace(go.Scatter(
    x=df_similaridades['NU_PARAM_B'],
    y=df_similaridades['gabarito_similarity'],
    mode='markers',
    marker=dict(
        size=7,
    ),
    hoverinfo='text',
))

fig.update_layout(
    title="Distribuição: média das similariades x Parâmetro B",
    xaxis_title="Parâmetro de dificuldade do item",
    yaxis_title="Média da similaridade entre alternativas",
    template="plotly_white"
)

fig.show()

"""---"""

#@title Similaridade entre as respostas erradas e o enunciado

def average_wrong_similarity(row, alternatives_columns):
    correct_letter = row["TX_GABARITO"].strip().lower()
    if correct_letter.upper() not in alternatives_columns:
        return np.nan
    correct_text = row[correct_letter.upper()]
    emb_correct = get_bert_embedding(correct_text, bert_model)
    similarities = []
    for col in alternatives_columns:
        if col.lower() != correct_letter:
            alt_text = row[col]
            if isinstance(alt_text, str) and alt_text.strip():
                emb_alt = get_bert_embedding(alt_text, bert_model)
                norm_correct = np.linalg.norm(emb_correct)
                norm_alt = np.linalg.norm(emb_alt)
                if norm_correct > 0 and norm_alt > 0:
                    sim = np.dot(emb_correct, emb_alt) / (norm_correct * norm_alt)
                    similarities.append(sim)
    return np.mean(similarities) if similarities else np.nan


df_similaridades["avg_wrong_similarity"] = df_similaridades.apply(lambda row: average_wrong_similarity(row, alternatives_columns), axis=1)
df_similaridades[["TX_GABARITO", "avg_wrong_similarity"]].head()

#@title Correlação similaridade enunciado/respostas erradas x Parâmetro B

df_similaridades[['avg_wrong_similarity', 'NU_PARAM_B']].corr()

#@title Scatterplot: média das similariades das respostas erradas x Parâmetro B


fig = go.Figure()

fig.add_trace(go.Scatter(
    x=df_similaridades['NU_PARAM_B'],
    y=df_similaridades['avg_wrong_similarity'],
    mode='markers',
    marker=dict(
        size=7,
    ),
    hoverinfo='text',
))

fig.update_layout(
    title="Distribuição: média das similariades enunciado/respostas erradas x Parâmetro B",
    xaxis_title="Parâmetro de dificuldade do item",
    yaxis_title="Média da similaridade das respostas erradas com o enunciado",
    template="plotly_white"
)

fig.show()

"""---"""

#@title Diferença entre as similaridades enunciado/gabarito e enunciado/respostas erradas

df_similaridades["diff_similarity"] = df_similaridades["gabarito_similarity"] - df_similaridades["avg_wrong_similarity"]

sns.set_style("darkgrid")

sns.histplot(df_similaridades['diff_similarity'], bins = 20)

#@title Scatterplot

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=df_similaridades['NU_PARAM_B'],
    y=df_similaridades['diff_similarity'],
    mode='markers',
    marker=dict(
        size=7
    ),
    hoverinfo='text'
))

fig.update_layout(
    title="Distribuição: diferença entre as médias de similaridade x Parâmetro B",
    xaxis_title="Parâmetro de dificuldade do item",
    yaxis_title="Diferença entre as similaridades (resposta errada/certa) x enunciado",
    template="plotly_white"
)

fig.show()

"""---

# **5. Regressão a partir dos embbedings**

### **a) Word2Vec**
"""

#@title Filtro: valores entre -3 e 3.

df_regressao = df.copy()
print(df_regressao.shape)
df_regressao = df_regressao.dropna(subset=['NU_PARAM_B'])
df_regressao = df_regressao[df_regressao['NU_PARAM_B'] >= -3]
df_regressao = df_regressao[df_regressao['NU_PARAM_B'] <= 3]
print(df_regressao.shape)

#@title Função de Pré-processamento dos Itens

# Para o protocolo Primi é necessário:
# 1. Dividir o item em palavras
# 2. Remover stopwords
# 3. Remover numerais, duplicatas e aplicar o lower

def preprocess_item(text):
    doc = nlp(text)
    words = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]
    seen = set()
    unique_words = []
    for word in words:
        if word not in seen:
            unique_words.append(word)
            seen.add(word)
    return unique_words

def get_avg_embedding(tokens, nlp_model=nlp_vectors):
    vectors = []
    for token in tokens:
        if nlp_model.vocab.has_vector(token):
            vectors.append(nlp_model.vocab[token].vector)
    if vectors:
        return np.mean(vectors, axis=0)
    return np.zeros(nlp_model.vocab.vectors_length)

"""---
### **Word2Vec Embeddings**
"""

#@title Com pré processamento

word2vec_df = pd.DataFrame()
word2vec_df['enunciado'] = df_regressao['enunciado']
word2vec_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec_df['tokens'] = word2vec_df['enunciado'].astype(str).apply(preprocess_item)
word2vec_df['embeddings'] = word2vec_df["tokens"].apply(get_avg_embedding)

word2vec_train, word2vec_test = train_test_split(word2vec_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec_test['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

#@title Regressão Lasso com Grid-Search

X = np.vstack(word2vec_train['embeddings'].values)
y = np.array(word2vec_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao Train-test split pré-definido

lasso = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---"""

#@title Sem pré-processamento
word2vec2_df = pd.DataFrame()
word2vec2_df['enunciado'] = df_regressao['enunciado']
word2vec2_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec2_df['embeddings'] = word2vec2_df['enunciado'].astype(str).apply(get_embedding_enunciado)

word2vec2_df_train, word2vec2_df_test = train_test_split(word2vec2_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec2_df_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec2_df_test['embeddings'].values))


no_preprocess_reg = sm.OLS(word2vec2_df_train['NU_PARAM_B'], X_train)
results = no_preprocess_reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec2_df_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec2_df_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec2_df_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE:", mse)
print("Correlação:", corr)

#@title Grid search Lasso regression (sem pré-processamento)

X = np.vstack(word2vec2_df_train['embeddings'].values)
y = np.array(word2vec2_df_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao split pré-definido

lasso = sm.OLS(word2vec2_df_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec2_df_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec2_df_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec2_df_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---
### **BERT embeddings**
"""

#@title BERT embeddings para regressão

bert_regression = pd.DataFrame()
bert_regression['enunciado'] = df_regressao['enunciado']
bert_regression['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
bert_regression['embeddings'] = bert_regression['enunciado'].astype(str).apply(lambda x: get_bert_embedding(model = bert_model, text = x))

bert_regression_train, bert_regression_test = train_test_split(bert_regression, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(bert_regression_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(bert_regression_test['embeddings'].values))

reg = sm.OLS(bert_regression_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(bert_regression_test['NU_PARAM_B'], y_pred)
r2 = r2_score(bert_regression_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(bert_regression_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE:", mse)
print("Correlação:", corr)

#@title Grid search Lasso Regression (BERT embeddings)

X = np.vstack(bert_regression_train['embeddings'].values)
y = np.vstack(bert_regression_train['NU_PARAM_B'].values)

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao split pré-definido

lasso = sm.OLS(bert_regression_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(bert_regression_test['NU_PARAM_B'], y_pred)
r2 = r2_score(bert_regression_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(bert_regression_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---
## **Testando outros modelos**

### **CBOW 100 dimensões**
"""

!python -m spacy init vectors pt "/content/drive/MyDrive/Mineração Estatística/NILC - Embeddings/cbow_s100.txt" output_vectors
nlp_vectors = spacy.load("output_vectors")
print("Dimensão dos vetores:", nlp_vectors.vocab.vectors_length)

#@title Com pré processamento

word2vec_df = pd.DataFrame()
word2vec_df['enunciado'] = df_regressao['enunciado']
word2vec_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec_df['tokens'] = word2vec_df['enunciado'].astype(str).apply(preprocess_item)
word2vec_df['embeddings'] = word2vec_df["tokens"].apply(get_avg_embedding)

word2vec_train, word2vec_test = train_test_split(word2vec_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec_test['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

#@title Regressão Lasso com Grid-Search

X = np.vstack(word2vec_train['embeddings'].values)
y = np.array(word2vec_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao Train-test split pré-definido

lasso = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---
### **CBOW 50 dimensões**
"""

!python -m spacy init vectors pt "/content/drive/MyDrive/Mineração Estatística/NILC - Embeddings/cbow_s50.txt" output_vectors
nlp_vectors = spacy.load("output_vectors")
print("Dimensão dos vetores:", nlp_vectors.vocab.vectors_length)

#@title Com pré processamento

word2vec_df = pd.DataFrame()
word2vec_df['enunciado'] = df_regressao['enunciado']
word2vec_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec_df['tokens'] = word2vec_df['enunciado'].astype(str).apply(preprocess_item)
word2vec_df['embeddings'] = word2vec_df["tokens"].apply(get_avg_embedding)

word2vec_train, word2vec_test = train_test_split(word2vec_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec_test['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

#@title Regressão Lasso com Grid-Search

X = np.vstack(word2vec_train['embeddings'].values)
y = np.array(word2vec_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao Train-test split pré-definido

lasso = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---
### **GloVe 300 dimensões**
"""

!python -m spacy init vectors pt "/content/drive/MyDrive/Mineração Estatística/NILC - Embeddings/glove_s300.txt" output_vectors
nlp_vectors = spacy.load("output_vectors")
print("Dimensão dos vetores:", nlp_vectors.vocab.vectors_length)

#@title Com pré processamento

word2vec_df = pd.DataFrame()
word2vec_df['enunciado'] = df_regressao['enunciado']
word2vec_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec_df['tokens'] = word2vec_df['enunciado'].astype(str).apply(preprocess_item)
word2vec_df['embeddings'] = word2vec_df["tokens"].apply(get_avg_embedding)

word2vec_train, word2vec_test = train_test_split(word2vec_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec_test['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

#@title Regressão Lasso com Grid-Search

X = np.vstack(word2vec_train['embeddings'].values)
y = np.array(word2vec_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao Train-test split pré-definido

lasso = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---
### **GloVe 100 dimensões**
"""

!python -m spacy init vectors pt "/content/drive/MyDrive/Mineração Estatística/NILC - Embeddings/glove_s100.txt" output_vectors
nlp_vectors = spacy.load("output_vectors")
print("Dimensão dos vetores:", nlp_vectors.vocab.vectors_length)

#@title Com pré processamento

word2vec_df = pd.DataFrame()
word2vec_df['enunciado'] = df_regressao['enunciado']
word2vec_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec_df['tokens'] = word2vec_df['enunciado'].astype(str).apply(preprocess_item)
word2vec_df['embeddings'] = word2vec_df["tokens"].apply(get_avg_embedding)

word2vec_train, word2vec_test = train_test_split(word2vec_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec_test['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

#@title Regressão Lasso com Grid-Search

X = np.vstack(word2vec_train['embeddings'].values)
y = np.array(word2vec_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao Train-test split pré-definido

lasso = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---
### **GloVe 50 dimensões**
"""

!python -m spacy init vectors pt "/content/drive/MyDrive/Mineração Estatística/NILC - Embeddings/glove_s50.txt" output_vectors
nlp_vectors = spacy.load("output_vectors")
print("Dimensão dos vetores:", nlp_vectors.vocab.vectors_length)

#@title Com pré processamento

word2vec_df = pd.DataFrame()
word2vec_df['enunciado'] = df_regressao['enunciado']
word2vec_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec_df['tokens'] = word2vec_df['enunciado'].astype(str).apply(preprocess_item)
word2vec_df['embeddings'] = word2vec_df["tokens"].apply(get_avg_embedding)

word2vec_train, word2vec_test = train_test_split(word2vec_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec_test['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

#@title Regressão Lasso com Grid-Search

X = np.vstack(word2vec_train['embeddings'].values)
y = np.array(word2vec_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao Train-test split pré-definido

lasso = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---
### **SkipGram 300 dimensões**
"""

!python -m spacy init vectors pt "/content/drive/MyDrive/Mineração Estatística/NILC - Embeddings/skip_s300.txt" output_vectors
nlp_vectors = spacy.load("output_vectors")
print("Dimensão dos vetores:", nlp_vectors.vocab.vectors_length)

#@title Com pré processamento

word2vec_df = pd.DataFrame()
word2vec_df['enunciado'] = df_regressao['enunciado']
word2vec_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec_df['tokens'] = word2vec_df['enunciado'].astype(str).apply(preprocess_item)
word2vec_df['embeddings'] = word2vec_df["tokens"].apply(get_avg_embedding)

word2vec_train, word2vec_test = train_test_split(word2vec_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec_test['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

#@title Regressão Lasso com Grid-Search

X = np.vstack(word2vec_train['embeddings'].values)
y = np.array(word2vec_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao Train-test split pré-definido

lasso = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---
# **6) Análise dos embeddings**

## **Item de maior dificuldade**
"""

df_regressao['NU_PARAM_B'].max()

df_regressao[df_regressao['NU_PARAM_B'] == 2.99298]

"""## **Item de menor dificuldade**"""

df_regressao['NU_PARAM_B'].min()

df_regressao[df_regressao['NU_PARAM_B'] == -1.80092]

df_regressao[df_regressao['ANO'] == 2011]

"""## **Nuvens de palavras**"""

#@title Dimensões com maior contribuição positiva e negativa

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
reg = reg.fit()

coef = reg.params
max_pos_dim = np.argmax(coef)
max_neg_dim = np.argmin(coef)
print("Dimensão com maior contribuição positiva:", max_pos_dim, "| Coeficiente:", coef[max_pos_dim])
print("Dimensão com maior contribuição negativa:", max_neg_dim, "| Coeficiente:", coef[max_neg_dim])

df['enunciado'] = df['enunciado'].astype(str)

def get_embedding_word(word, nlp_model=nlp_vectors):
    doc = nlp_model(word.lower())
    if doc and doc[0].has_vector:
        return doc[0].vector
    return np.zeros(nlp_model.vocab.vectors_length)

vocab = set()
for text in df["enunciado"]:
    doc = nlp_vectors(text.lower())
    vocab.update([token.text for token in doc if token.is_alpha])
vocab = list(vocab)

scores_pos = {}
for word in vocab:
    if nlp_vectors.vocab.has_vector(word):
        emb = get_embedding_word(word, nlp_vectors)
        scores_pos[word] = emb[max_pos_dim]
top_100_pos = dict(sorted(scores_pos.items(), key=lambda x: x[1], reverse=True)[:100])


scores_neg = {}
for word in vocab:
    if nlp_vectors.vocab.has_vector(word):
        emb = get_embedding_word(word, nlp_vectors)
        scores_neg[word] = emb[max_neg_dim]
top_100_neg = dict(sorted(scores_neg.items(), key=lambda x: x[1])[:100])

#@title Wordcloud contribuição Negativa (mais difíceis)

wc_pos = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_100_pos)
plt.figure(figsize=(10, 5))
plt.imshow(wc_pos, interpolation='bilinear')
plt.axis("off")
plt.title("Nuvem de Palavras - Dimensão com Contribuição Positiva")
plt.show()

#@title Wordcloud contribuição Negativa (mais fáceis)

wc_neg = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_100_neg)
plt.figure(figsize=(10, 5))
plt.imshow(wc_neg, interpolation='bilinear')
plt.axis("off")
plt.title("Nuvem de Palavras - Dimensão com Contribuição Negativa")
plt.show()

"""---
# **7) Fine tuned BERT**
"""

#@title Torch imports
!pip install datasets

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
import torch.nn as nn
import tqdm
import transformers as ppb
from transformers import AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments, AutoModelForSequenceClassification
from datasets import Dataset

"""## **Classes de regressão e early stopping**

#### **Reinitialize layers e AdamW**



```
Uma boa prática ao realizar o fine tuning de arquiteturas Transformer é reinicializar as camadas próximas à
saída. Além disso, substitui-se o otimizador AdamBERT pelo AdamW, que integra um termo de correção de viés na
atualização dos pesos
```
"""

#@title Model e tokenizer

model = ppb.AutoModelForSequenceClassification.from_pretrained(
    'neuralmind/bert-base-portuguese-cased',
    num_labels=1,
    problem_type="regression",
    output_attentions=False,
    output_hidden_states=False
)


def reinitialize_last_layers(model, num_layers=2):
    for layer in model.bert.encoder.layer[-num_layers:]:
        for module in layer.modules():
            if isinstance(module, nn.Linear):
                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)
                if module.bias is not None:
                    module.bias.data.zero_()
            elif isinstance(module, nn.Embedding):
                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)
            elif isinstance(module, nn.LayerNorm):
                module.bias.data.zero_()
                module.weight.data.fill_(1.0)
    if hasattr(model, "classifier"):
        model.classifier.weight.data.normal_(mean=0.0, std=model.config.initializer_range)
        model.classifier.bias.data.zero_()


reinitialize_last_layers(model, 2)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=2.26e-5, eps=5e-08, weight_decay=0.03)
tokenizer = ppb.AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)

#@title Pre-processamento

train_df, test_df = train_test_split(df_regressao, test_size=0.2, random_state=24)

def preprocessing(input_text, tokenizer):
    return tokenizer.encode_plus(
        input_text,
        add_special_tokens=True,
        max_length=512,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt'
    )


def tokenize_dataframe(df, tokenizer):
    tokenized = []
    attention_masks = []
    for sample in df['enunciado'].values:
        encoding_dict = preprocessing(sample, tokenizer)
        tokenized.append(encoding_dict['input_ids'])
        attention_masks.append(encoding_dict['attention_mask'])
    tokenized = torch.cat(tokenized, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    return tokenized, attention_masks

train_input_ids, train_attention_masks = tokenize_dataframe(train_df, tokenizer)
test_input_ids, test_attention_masks = tokenize_dataframe(test_df, tokenizer)

# Conversão para float está dando errado, ajustar eventualmente
train_labels = torch.tensor(train_df['NU_PARAM_B'].values, dtype=torch.float)
test_labels = torch.tensor(test_df['NU_PARAM_B'].values, dtype=torch.float)


train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)
test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)

train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)
test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=16)

#@title Treinamento com early stopping class

num_epochs = 3
best_val_loss = float('inf')
patience = 2
early_stop_counter = 0

for epoch in range(num_epochs):
    model.train()
    total_train_loss = 0
    for batch in tqdm.tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs} training"):
        b_input_ids, b_input_mask, b_labels = (t.to(device) for t in batch)
        optimizer.zero_grad()

        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels.unsqueeze(1))
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_dataloader)

    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for batch in tqdm.tqdm(test_dataloader, desc="Validation"):
            b_input_ids, b_input_mask, b_labels = (t.to(device) for t in batch)
            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels.unsqueeze(1))
            total_val_loss += outputs.loss.item()
    avg_val_loss = total_val_loss / len(test_dataloader)
    print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        early_stop_counter = 0
    else:
        early_stop_counter += 1
        if early_stop_counter >= patience:
            print("Early stopping!")
            break

#@title Eval
model.eval()
y_preds = []
y_true = []
with torch.no_grad():
    for batch in test_dataloader:
        b_input_ids, b_input_mask, b_labels = (t.to(device) for t in batch)
        outputs = model(b_input_ids, attention_mask=b_input_mask)
        preds = outputs.logits.squeeze(1)  # shape: (batch_size)
        y_preds.extend(preds.cpu().numpy())
        y_true.extend(b_labels.cpu().numpy())

y_preds = np.array(y_preds)
y_true = np.array(y_true)

from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_true, y_preds)
r2 = r2_score(y_true, y_preds)
corr = np.corrcoef(y_true, y_preds)[0, 1]

print("MSE:", mse)
print("Correlação:", corr)

"""---

## **Sem reinicializar as primeiras camadas**

---
## **Otimizando hiperparâmetros**
"""

#@title Dependências
!pip install optuna
import optuna

#@title Optuna

model = "neuralmind/bert-base-portuguese-cased"
tokenizer = AutoTokenizer.from_pretrained(model)
data_collator = DataCollatorWithPadding(tokenizer, padding="longest")

train_df, test_df = train_test_split(df_regressao, test_size=0.2, random_state=24)

train_ds = Dataset.from_pandas(train_df.rename(columns={"NU_PARAM_B":"label"}))
eval_ds  = Dataset.from_pandas(test_df.rename(columns={"NU_PARAM_B":"label"}))

def tokenize_function(examples):
    return tokenizer(examples["enunciado"],
                     truncation=True,
                     max_length=512,
                     padding=False)

train_ds = train_ds.map(tokenize_function, batched=True)
eval_ds  = eval_ds.map(tokenize_function,  batched=True)

def compute_metrics(p):
    preds  = p.predictions.squeeze()
    labels = p.label_ids
    corr   = np.corrcoef(labels, preds)[0,1]
    mse    = mean_squared_error(labels, preds)
    return {"correlation": corr, "mse": mse}

"""def compute_objective(trial):
    preds = trial.predict.squeeze()
    labels = trial.label_ids

    mse = mean_squared_error(labels, preds)

    return mse"""

def model_init():

    model = AutoModelForSequenceClassification.from_pretrained(
        "neuralmind/bert-base-portuguese-cased",
        num_labels=1
    )

    reinitialize_last_layers(model, num_layers=2)

    return model

# Não salva os logs
training_args = TrainingArguments(
    output_dir="./dummy",
    do_train=True,
    do_eval=True,
    save_strategy="no",
    logging_strategy="no",
    report_to=[],
    disable_tqdm=False,
)

trainer = Trainer(
    model_init     = model_init,
    args           = training_args,
    train_dataset  = train_ds,
    eval_dataset   = eval_ds,
    tokenizer      = tokenizer,
    data_collator  = data_collator,
    compute_metrics= compute_metrics
)

def hp_space(trial: optuna.Trial):
    return {
        "learning_rate":  trial.suggest_loguniform("learning_rate", 1e-5, 5e-5),
        "weight_decay":   trial.suggest_float("weight_decay",    0.0,   0.2),
    }

best_run = trainer.hyperparameter_search(
    direction        = "minimize",
    backend          = "optuna",
    hp_space         = hp_space,
    n_trials         = 20,
    compute_objective = lambda metrics: metrics["eval_mse"]
)

print("Melhor learning_rate :", best_run.hyperparameters["learning_rate"])
print("Melhor weight_decay :", best_run.hyperparameters["weight_decay"])
print("Melhor correlação   :", best_run.objective)

#@title Calculando métricas de avaliação

best_hps = best_run.hyperparameters
best_lr = best_hps["learning_rate"]
best_wd = best_hps["weight_decay"]

training_args = TrainingArguments(
    output_dir="./dummy",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=best_lr,
    weight_decay=best_wd,
    disable_tqdm=False,
    report_to='none'
)


trainer = Trainer(
    model_init     = model_init,
    args           = training_args,
    train_dataset  = train_ds,
    eval_dataset   = eval_ds,
    tokenizer      = tokenizer,
    data_collator  = data_collator,
    compute_metrics= compute_metrics
)

trainer.train()

predictions_output = trainer.predict(eval_ds)
preds = predictions_output.predictions.squeeze()   # array de floats
labels = predictions_output.label_ids


print("MSE final:", mean_squared_error(labels, preds))
print("R² final: ", r2_score(labels, preds))
print("Corr final:", np.corrcoef(labels, preds)[0,1])

"""---
### **Multilingual BERT**
"""

#@title Optuna

model = "google-bert/bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(model)
data_collator = DataCollatorWithPadding(tokenizer, padding="longest")

train_df, test_df = train_test_split(df_regressao, test_size=0.2, random_state=24)

train_ds = Dataset.from_pandas(train_df.rename(columns={"NU_PARAM_B":"label"}))
eval_ds  = Dataset.from_pandas(test_df.rename(columns={"NU_PARAM_B":"label"}))

def tokenize_function(examples):
    return tokenizer(examples["enunciado"],
                     truncation=True,
                     max_length=512,
                     padding=False)

train_ds = train_ds.map(tokenize_function, batched=True)
eval_ds  = eval_ds.map(tokenize_function,  batched=True)

def compute_metrics(p):
    preds  = p.predictions.squeeze()
    labels = p.label_ids
    corr   = np.corrcoef(labels, preds)[0,1]
    mse    = mean_squared_error(labels, preds)
    return {"correlation": corr, "mse": mse}

"""def compute_objective(trial):
    preds = trial.predict.squeeze()
    labels = trial.label_ids

    mse = mean_squared_error(labels, preds)

    return mse"""

def model_init():

    model = AutoModelForSequenceClassification.from_pretrained(
        "google-bert/bert-base-multilingual-cased",
        num_labels=1
    )

    reinitialize_last_layers(model, num_layers=2)

    return model

training_args = TrainingArguments(
    output_dir="./dummy",
    do_train=True,
    do_eval=True,
    save_strategy="no",
    logging_strategy="no",
    report_to=[],
    disable_tqdm=False,
)

trainer = Trainer(
    model_init     = model_init,
    args           = training_args,
    train_dataset  = train_ds,
    eval_dataset   = eval_ds,
    tokenizer      = tokenizer,
    data_collator  = data_collator,
    compute_metrics= compute_metrics
)

def hp_space(trial: optuna.Trial):
    return {
        "learning_rate":  trial.suggest_loguniform("learning_rate", 1e-5, 5e-5),
        "weight_decay":   trial.suggest_float("weight_decay",    0.0,   0.2),
    }

best_run = trainer.hyperparameter_search(
    direction        = "minimize",
    backend          = "optuna",
    hp_space         = hp_space,
    n_trials         = 20,
    compute_objective = lambda metrics: metrics["eval_mse"]
)

print("Melhor learning_rate :", best_run.hyperparameters["learning_rate"])
print("Melhor weight_decay :", best_run.hyperparameters["weight_decay"])
print("Melhor correlação   :", best_run.objective)

#@title Extraindo métricas de avaliação

def model_init():

    model = AutoModelForSequenceClassification.from_pretrained(
        "google-bert/bert-base-multilingual-cased",
        num_labels=1
    )

    reinitialize_last_layers(model, num_layers=2)

    return model

best_hps = best_run.hyperparameters
best_lr = best_hps["learning_rate"]
best_wd = best_hps["weight_decay"]

training_args = TrainingArguments(
    output_dir="./dummy",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=best_lr,
    weight_decay=best_wd,
    disable_tqdm=False,
    report_to='none'
)


trainer = Trainer(
    model_init     = model_init,
    args           = training_args,
    train_dataset  = train_ds,
    eval_dataset   = eval_ds,
    tokenizer      = tokenizer,
    data_collator  = data_collator,
    compute_metrics= compute_metrics
)

trainer.train()

predictions_output = trainer.predict(eval_ds)
preds = predictions_output.predictions.squeeze()   # array de floats
labels = predictions_output.label_ids


print("MSE final:", mean_squared_error(labels, preds))
print("R² final: ", r2_score(labels, preds))
print("Corr final:", np.corrcoef(labels, preds)[0,1])
