# -*- coding: utf-8 -*-
"""Análise_embedings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hk-vfUk-9atqPRIQJiIDJyeabnAvRW-Q

```
Augusto Sousa Nunes - NºUSP: 11319677
Lua Nardi Quito - NºUSP: 11371270
Lucas Schimidt Coelho - NºUSP: 11913019
Lucas Garzeri de Melo - NºUSP: 13731344
Lucca Baptista Silva Ferraz - NºUSP: 13688134
```

#Imports e dados
"""

# Dependencias base
import pandas as pd
import  matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import re
import statsmodels.api as sm

# Análise linguística
!python -m spacy download pt_core_news_sm
import spacy
nlp = spacy.load("pt_core_news_sm")
!pip install pyphen
import pyphen

# Embeddings - o gensim só é compatível com versões antigas do numpy, utiliza-lo apenas quando for mexer com os embeddings word2vec
'''!pip install numpy==1.24.3
import numpy as np
!pip install -q --upgrade gensim
from gensim.models import KeyedVectors'''
# Bertimbau
import numpy as np
from sentence_transformers import SentenceTransformer

# UMAP para visualização - só funciona com versões atualizadas do numpy, usar apenas quando não for mexer nos embeddings do gensim
!pip install umap-learn
import umap

# Métricas
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

nlp = spacy.load("pt_core_news_sm")
dic = pyphen.Pyphen(lang='pt_BR')

# Drive
from google.colab import drive
drive.mount('/content/drive', force_remount = True)

# Warnings
import warnings
warnings.filterwarnings('ignore')


# Wordcloud
from wordcloud import WordCloud

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

"""---

## **Base de dados estruturada**
"""

#@title Extração de dados

df = pd.read_csv('/content/drive/MyDrive/Dados ENEM Mineração/Base - Lucca/csv_final.csv')
df.head()

df.info()

"""---

---
# **Análise dos embeddings**

## **Item de maior dificuldade**
"""

df_regressao['NU_PARAM_B'].max()

df_regressao[df_regressao['NU_PARAM_B'] == 2.99298]

"""## **Item de menor dificuldade**"""

df_regressao['NU_PARAM_B'].min()

df_regressao[df_regressao['NU_PARAM_B'] == -1.80092]

df_regressao[df_regressao['ANO'] == 2011]

"""## **Nuvens de palavras**"""

#@title Dimensões com maior contribuição positiva e negativa

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
reg = reg.fit()

coef = reg.params
max_pos_dim = np.argmax(coef)
max_neg_dim = np.argmin(coef)
print("Dimensão com maior contribuição positiva:", max_pos_dim, "| Coeficiente:", coef[max_pos_dim])
print("Dimensão com maior contribuição negativa:", max_neg_dim, "| Coeficiente:", coef[max_neg_dim])

df['enunciado'] = df['enunciado'].astype(str)

def get_embedding_word(word, nlp_model=nlp_vectors):
    doc = nlp_model(word.lower())
    if doc and doc[0].has_vector:
        return doc[0].vector
    return np.zeros(nlp_model.vocab.vectors_length)

vocab = set()
for text in df["enunciado"]:
    doc = nlp_vectors(text.lower())
    vocab.update([token.text for token in doc if token.is_alpha])
vocab = list(vocab)

scores_pos = {}
for word in vocab:
    if nlp_vectors.vocab.has_vector(word):
        emb = get_embedding_word(word, nlp_vectors)
        scores_pos[word] = emb[max_pos_dim]
top_100_pos = dict(sorted(scores_pos.items(), key=lambda x: x[1], reverse=True)[:100])


scores_neg = {}
for word in vocab:
    if nlp_vectors.vocab.has_vector(word):
        emb = get_embedding_word(word, nlp_vectors)
        scores_neg[word] = emb[max_neg_dim]
top_100_neg = dict(sorted(scores_neg.items(), key=lambda x: x[1])[:100])

#@title Wordcloud contribuição Negativa (mais difíceis)

wc_pos = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_100_pos)
plt.figure(figsize=(10, 5))
plt.imshow(wc_pos, interpolation='bilinear')
plt.axis("off")
plt.title("Nuvem de Palavras - Dimensão com Contribuição Positiva")
plt.show()

#@title Wordcloud contribuição Negativa (mais fáceis)

wc_neg = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_100_neg)
plt.figure(figsize=(10, 5))
plt.imshow(wc_neg, interpolation='bilinear')
plt.axis("off")
plt.title("Nuvem de Palavras - Dimensão com Contribuição Negativa")
plt.show()