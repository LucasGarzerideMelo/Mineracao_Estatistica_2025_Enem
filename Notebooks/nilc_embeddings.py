# -*- coding: utf-8 -*-
"""NILC_embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-CFYhRdwfx-oPvu7p6h56E04k0qgPeEo

```
Augusto Sousa Nunes - NºUSP: 11319677
Lua Nardi Quito - NºUSP: 11371270
Lucas Schimidt Coelho - NºUSP: 11913019
Lucas Garzeri de Melo - NºUSP: 13731344
Lucca Baptista Silva Ferraz - NºUSP: 13688134
```

#Imports e dados
"""

# Dependencias base
import pandas as pd
import  matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import re
import statsmodels.api as sm

# Análise linguística
!python -m spacy download pt_core_news_sm
import spacy
nlp = spacy.load("pt_core_news_sm")
!pip install pyphen
import pyphen

# Embeddings - o gensim só é compatível com versões antigas do numpy, utiliza-lo apenas quando for mexer com os embeddings word2vec
'''!pip install numpy==1.24.3
import numpy as np
!pip install -q --upgrade gensim
from gensim.models import KeyedVectors'''
# Bertimbau
import numpy as np
from sentence_transformers import SentenceTransformer

# UMAP para visualização - só funciona com versões atualizadas do numpy, usar apenas quando não for mexer nos embeddings do gensim
!pip install umap-learn
import umap

# Métricas
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

nlp = spacy.load("pt_core_news_sm")
dic = pyphen.Pyphen(lang='pt_BR')

# Drive
from google.colab import drive
drive.mount('/content/drive', force_remount = True)

# Warnings
import warnings
warnings.filterwarnings('ignore')


# Wordcloud
from wordcloud import WordCloud

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

"""---

## **Base de dados estruturada**
"""

#@title Extração de dados

df = pd.read_csv('/content/drive/MyDrive/Dados ENEM Mineração/Base - Lucca/csv_final.csv')
df.head()

df.info()

"""# **NILC Embeddings**

Conforme a metodologia do artigo, os enunciados das questões do Enem foram vetorizados a partir do algoritmo Word2Vec, extraindo a média dos vetores de palavra para cada item. O algoritmo Word2Vec é uma técnica de aprendizado de representações vetoriais para palavras, cujo funcionamento depende de uma rede neural rasa que recebe a tarefa de reconstruir contexto linguístico. Assim, os modelos aprendem a criar representações de alta dimensão que preservam a semântica das palavras de forma implícita.

\\

**Existem duas arquiteturas principais em Word2Vec:**

#### *CBOW (Continuous Bag of Words):*
Nesse modelo, o algoritmo prevê a palavra central a partir das palavras de contexto ao redor. Assim, o modelo aprende quais palavras costumam aparecer juntas, aproximando os vetores de palavras que compartilham contextos similares.

#### *Skip-gram:*
Nesse caso, o processo é invertido: a partir de uma palavra central, o modelo tenta prever as palavras que aparecem em seu contexto. Essa abordagem é especialmente eficaz para capturar relações semânticas em conjuntos de dados menores.

Todos os vetores aqui utilizados foram retirados do repositório do NILC.

[1] *HARTMANN, Nathan; FONSECA, Erick; SHULBY, Christopher; TREVISO, Marcos; RODRIGUES, Jessica; ALUISIO, Sandra. Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks. 2017. Preprint. Disponível em: https://arxiv.org/abs/1708.06025.*
"""

!python -m spacy init vectors pt "/content/drive/MyDrive/Mineração Estatística/NILC - Embeddings/cbow_s300.txt" output_vectors

nlp_vectors = spacy.load("output_vectors")
print("Dimensão dos vetores:", nlp_vectors.vocab.vectors_length)

#@title Gerar embeddings dos enunciados


def get_embedding_enunciado(text, nlp_model = nlp_vectors):
    doc = nlp_model(text.lower())
    vectors = [token.vector for token in doc if token.has_vector]
    return np.mean(vectors, axis=0) if vectors else np.zeros(nlp_model.vocab.vectors_length)

#@title Vetorização segundo o protocolo primi

def preprocess_item(text):
    doc = nlp(text)
    words = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]
    seen = set()
    unique_words = []
    for word in words:
        if word not in seen:
            unique_words.append(word)
            seen.add(word)
    return unique_words

def get_avg_embedding(tokens, nlp_model=nlp_vectors):
    vectors = []
    for token in tokens:
        if nlp_model.vocab.has_vector(token):
            vectors.append(nlp_model.vocab[token].vector)
    if vectors:
        return np.mean(vectors, axis=0)
    return np.zeros(nlp_model.vocab.vectors_length)

#@title Aplicando à coluna "Enunciados"

embeddings_df = pd.DataFrame()
embeddings_df['enunciado'] = df['enunciado']
embeddings_df['NU_PARAM_B'] = df['NU_PARAM_B']
embeddings_df['embeddings'] = embeddings_df['enunciado'].astype(str).apply(get_embedding_enunciado)
embeddings_df

"""## **Uniform Manifold Approximation and Projection (UMAP)**

O UMAP é um método de Redução de dimensionalidade que visa preservar cluster de altas dimensões em um gráfico de baixa dimensão. Facilita-se, pois, a visualização dos embeddings gerados a partir do Word2Vec, uma vez que a codificação do parâmetro de dificuldade é feita através da cor dos pontos no espaço de dimensionalidade reduzida.
"""

#@title Aplicando o UMAP

df_filtered = embeddings_df[(embeddings_df["NU_PARAM_B"] >= -3) & (embeddings_df["NU_PARAM_B"] <= 3)].copy()
print("Número de questões filtradas:", len(df_filtered))

embeddings_matrix = np.vstack(df_filtered["embeddings"].values)
print("Dimensão dos embeddings filtrados:", embeddings_matrix.shape)

reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
umap_embeddings = reducer.fit_transform(embeddings_matrix)
print("Dimensão reduzida:", umap_embeddings.shape)

#@title UMAP Scatterplot
import plotly.graph_objects as go

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=umap_embeddings[:, 0],
    y=umap_embeddings[:, 1],
    mode='markers',
    marker=dict(
        size=7,
        color=df_filtered['NU_PARAM_B'],
        colorscale= 'RdBu',
        colorbar=dict(title='NU_PARAM_B')
    ),
    hoverinfo='text',
    text=embeddings_df['NU_PARAM_B'].astype(str)
))

fig.update_layout(
    title="Projeção UMAP dos Embeddings",
    xaxis_title="UMAP 1",
    yaxis_title="UMAP 2",
    template="plotly_white"
)

fig.show()

"""---"""