# -*- coding: utf-8 -*-
"""Bert_embddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gfMa7r6iQjwrY3BUh1nhxW_ej4UlH1MT

```
Augusto Sousa Nunes - NºUSP: 11319677
Lua Nardi Quito - NºUSP: 11371270
Lucas Schimidt Coelho - NºUSP: 11913019
Lucas Garzeri de Melo - NºUSP: 13731344
Lucca Baptista Silva Ferraz - NºUSP: 13688134
```

#Imports e dados
"""

# Dependencias base
import pandas as pd
import  matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import re
import statsmodels.api as sm

# Análise linguística
!python -m spacy download pt_core_news_sm
import spacy
nlp = spacy.load("pt_core_news_sm")
!pip install pyphen
import pyphen

# Embeddings - o gensim só é compatível com versões antigas do numpy, utiliza-lo apenas quando for mexer com os embeddings word2vec
'''!pip install numpy==1.24.3
import numpy as np
!pip install -q --upgrade gensim
from gensim.models import KeyedVectors'''
# Bertimbau
import numpy as np
from sentence_transformers import SentenceTransformer

# UMAP para visualização - só funciona com versões atualizadas do numpy, usar apenas quando não for mexer nos embeddings do gensim
!pip install umap-learn
import umap

# Métricas
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

nlp = spacy.load("pt_core_news_sm")
dic = pyphen.Pyphen(lang='pt_BR')

# Drive
from google.colab import drive
drive.mount('/content/drive', force_remount = True)

# Warnings
import warnings
warnings.filterwarnings('ignore')


# Wordcloud
from wordcloud import WordCloud

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

"""---

## **Base de dados estruturada**
"""

#@title Extração de dados

df = pd.read_csv('/content/drive/MyDrive/Dados ENEM Mineração/Base - Lucca/csv_final.csv')
df.head()

df.info()

"""---

# **BERT embeddings**

De maneira análoga à extração de embeddings realizada pelo Word2Vec, utilizou-se dos enunciados das questões através do BERT [Devlin et al., 2019] (Bidirectional Encoder Representaions from Transformers), uma família de modelos de linguagem projetada com a habilidade de capturar contexto em duas direções, superando a performance de modelos contemporâneos a ele. Aqui, vale o destaque para o fato de que o modelo é sentencial, isto é, não mais depende da média das dimensões dos vetores palavra a palavra, captando sentenças completas e, por consequência, relações de ordem e posição entre as palavras nas frases.

Neste estudo, mais especificamente, utiliza-se do Bertimbau [Souza et al., 2020], uma variante treinada especificamente para o português brasileiro, que se adequa melhor às nuances da língua e cultura brasileira. Os embeddings extraídos pelo BERT serão, posteriormente, utilizados como atributos para a regressão do parâmetro de dificuldade dos itens.


[1] *Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
 BERT: Pre-training of deep bidirectional transformers for
 language understanding. In Burstein, J., Doran, C., and
 Solorio, T., editors, Proceedings of the 2019 Conference
 of the NorthAmericanChapteroftheAssociationforCom
putational Linguistics: Human Language Technologies,
 Volume 1 (Long and Short Papers), pages 4171–4186,
 Minneapolis, Minnesota.*

[2] *Souza, F., Nogueira, R., and Lotufo, R. (2020). BERTim
bau: pretrained BERT models for Brazilian Portuguese.
 In Cerri, R. and Prati, R. C., editors, Proceedings
 of the 9th Brazilian Conference on Intelligent Systems,
 BRACIS,RioGrandedoSul, Brazil, October20-23, pages
 403–417, Cham. Springer International Publishing. DOI:
 10.1007/978-3-030-61377-8_28.*
"""

#@title Bert embeddings
bert_model = SentenceTransformer("neuralmind/bert-base-portuguese-cased")

def get_bert_embedding(text, model):
    return model.encode(text)

bert_embeddings = df.copy()
bert_embeddings['embeddings'] = bert_embeddings['enunciado'].astype(str).apply(lambda x: get_bert_embedding(x, bert_model))

#@title Aplicando o UMAP

df_filtered_bert = bert_embeddings[(bert_embeddings["NU_PARAM_B"] >= -3) & (bert_embeddings["NU_PARAM_B"] <= 3)].copy()
print("Número de questões filtradas:", len(df_filtered_bert))

embeddings_matrix = np.vstack(df_filtered_bert["embeddings"].values)
print("Dimensão dos embeddings filtrados:", embeddings_matrix.shape)

reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
umap_embeddings = reducer.fit_transform(embeddings_matrix)
print("Dimensão reduzida:", umap_embeddings.shape)

#@title UMAP Scatterplot

import plotly.graph_objects as go

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=umap_embeddings[:, 0],
    y=umap_embeddings[:, 1],
    mode='markers',
    marker=dict(
        size=7,
        color=df_filtered_bert['NU_PARAM_B'],
        colorscale= 'RdBu',
        colorbar=dict(title='NU_PARAM_B')
    ),
    hoverinfo='text',
    text=df_filtered_bert['NU_PARAM_B'].astype(str)
))

fig.update_layout(
    title="Projeção UMAP dos Embeddings",
    xaxis_title="UMAP 1",
    yaxis_title="UMAP 2",
    template="plotly_white"
)

fig.show()

"""---"""