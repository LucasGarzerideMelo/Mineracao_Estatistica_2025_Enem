# -*- coding: utf-8 -*-
"""Regressão_embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sC0U6ud9wKmcH193H5p_TREktVoWYhW1

```
Augusto Sousa Nunes - NºUSP: 11319677
Lua Nardi Quito - NºUSP: 11371270
Lucas Schimidt Coelho - NºUSP: 11913019
Lucas Garzeri de Melo - NºUSP: 13731344
Lucca Baptista Silva Ferraz - NºUSP: 13688134
```

#Imports e dados
"""

# Dependencias base
import pandas as pd
import  matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import re
import statsmodels.api as sm

# Análise linguística
!python -m spacy download pt_core_news_sm
import spacy
nlp = spacy.load("pt_core_news_sm")
!pip install pyphen
import pyphen

# Embeddings - o gensim só é compatível com versões antigas do numpy, utiliza-lo apenas quando for mexer com os embeddings word2vec
'''!pip install numpy==1.24.3
import numpy as np
!pip install -q --upgrade gensim
from gensim.models import KeyedVectors'''
# Bertimbau
import numpy as np
from sentence_transformers import SentenceTransformer

# UMAP para visualização - só funciona com versões atualizadas do numpy, usar apenas quando não for mexer nos embeddings do gensim
!pip install umap-learn
import umap

# Métricas
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

nlp = spacy.load("pt_core_news_sm")
dic = pyphen.Pyphen(lang='pt_BR')

# Drive
from google.colab import drive
drive.mount('/content/drive', force_remount = True)

# Warnings
import warnings
warnings.filterwarnings('ignore')


# Wordcloud
from wordcloud import WordCloud

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

"""---

## **Base de dados estruturada**
"""

#@title Extração de dados

df = pd.read_csv('/content/drive/MyDrive/Dados ENEM Mineração/Base - Lucca/csv_final.csv')
df.head()

df.info()

"""---

---

# **Regressão a partir dos embbedings**

### **a) Word2Vec**
"""

#@title Filtro: valores entre -3 e 3.

df_regressao = df.copy()
print(df_regressao.shape)
df_regressao = df_regressao.dropna(subset=['NU_PARAM_B'])
df_regressao = df_regressao[df_regressao['NU_PARAM_B'] >= -3]
df_regressao = df_regressao[df_regressao['NU_PARAM_B'] <= 3]
print(df_regressao.shape)

#@title Função de Pré-processamento dos Itens

# Para o protocolo Primi é necessário:
# 1. Dividir o item em palavras
# 2. Remover stopwords
# 3. Remover numerais, duplicatas e aplicar o lower

def preprocess_item(text):
    doc = nlp(text)
    words = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]
    seen = set()
    unique_words = []
    for word in words:
        if word not in seen:
            unique_words.append(word)
            seen.add(word)
    return unique_words

def get_avg_embedding(tokens, nlp_model=nlp_vectors):
    vectors = []
    for token in tokens:
        if nlp_model.vocab.has_vector(token):
            vectors.append(nlp_model.vocab[token].vector)
    if vectors:
        return np.mean(vectors, axis=0)
    return np.zeros(nlp_model.vocab.vectors_length)

"""---
### **Word2Vec Embeddings**
"""

#@title Com pré processamento

word2vec_df = pd.DataFrame()
word2vec_df['enunciado'] = df_regressao['enunciado']
word2vec_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec_df['tokens'] = word2vec_df['enunciado'].astype(str).apply(preprocess_item)
word2vec_df['embeddings'] = word2vec_df["tokens"].apply(get_avg_embedding)

word2vec_train, word2vec_test = train_test_split(word2vec_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec_test['embeddings'].values))

reg = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

#@title Regressão Lasso com Grid-Search

X = np.vstack(word2vec_train['embeddings'].values)
y = np.array(word2vec_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao Train-test split pré-definido

lasso = sm.OLS(word2vec_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---"""

#@title Sem pré-processamento
word2vec2_df = pd.DataFrame()
word2vec2_df['enunciado'] = df_regressao['enunciado']
word2vec2_df['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
word2vec2_df['embeddings'] = word2vec2_df['enunciado'].astype(str).apply(get_embedding_enunciado)

word2vec2_df_train, word2vec2_df_test = train_test_split(word2vec2_df, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(word2vec2_df_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(word2vec2_df_test['embeddings'].values))


no_preprocess_reg = sm.OLS(word2vec2_df_train['NU_PARAM_B'], X_train)
results = no_preprocess_reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec2_df_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec2_df_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec2_df_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE:", mse)
print("Correlação:", corr)

#@title Grid search Lasso regression (sem pré-processamento)

X = np.vstack(word2vec2_df_train['embeddings'].values)
y = np.array(word2vec2_df_train['NU_PARAM_B'])

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao split pré-definido

lasso = sm.OLS(word2vec2_df_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(word2vec2_df_test['NU_PARAM_B'], y_pred)
r2 = r2_score(word2vec2_df_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(word2vec2_df_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)

"""---
### **BERT embeddings**
"""

#@title BERT embeddings para regressão

bert_regression = pd.DataFrame()
bert_regression['enunciado'] = df_regressao['enunciado']
bert_regression['NU_PARAM_B'] = df_regressao['NU_PARAM_B']
bert_regression['embeddings'] = bert_regression['enunciado'].astype(str).apply(lambda x: get_bert_embedding(model = bert_model, text = x))

bert_regression_train, bert_regression_test = train_test_split(bert_regression, test_size=0.2, random_state=24)

X_train = sm.add_constant(np.vstack(bert_regression_train['embeddings'].values))
X_test = sm.add_constant(np.vstack(bert_regression_test['embeddings'].values))

reg = sm.OLS(bert_regression_train['NU_PARAM_B'], X_train)
results = reg.fit()
print(results.summary().tables[0])

y_pred = results.predict(X_test)

mse = mean_squared_error(bert_regression_test['NU_PARAM_B'], y_pred)
r2 = r2_score(bert_regression_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(bert_regression_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE:", mse)
print("Correlação:", corr)

#@title Grid search Lasso Regression (BERT embeddings)

X = np.vstack(bert_regression_train['embeddings'].values)
y = np.vstack(bert_regression_train['NU_PARAM_B'].values)

lasso = Lasso(max_iter=10_000, random_state=42)
param_grid = {
    'alpha': np.logspace(-4, 2, 50)
}

grid_mse = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid_mse.fit(X, y)

print("Melhor α:", grid_mse.best_params_['alpha'])
print("Melhor MSE (val):", -grid_mse.best_score_)

#@title Aplicando ao split pré-definido

lasso = sm.OLS(bert_regression_train['NU_PARAM_B'], X_train)
results = lasso.fit_regularized(alpha=grid_mse.best_params_['alpha'], L1_wt=1)

y_pred = results.predict(X_test)

mse = mean_squared_error(bert_regression_test['NU_PARAM_B'], y_pred)
r2 = r2_score(bert_regression_test['NU_PARAM_B'], y_pred)
corr = np.corrcoef(bert_regression_test['NU_PARAM_B'], y_pred)[0, 1]

print("MSE teste:", mse)
print("Correlação teste:", corr)